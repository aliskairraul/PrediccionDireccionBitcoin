{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2b73b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54408bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df: pl.DataFrame, columnas: list) -> pl.DataFrame:        \n",
    "    \"\"\"... (cÃ³digo original sin cambios) ...\"\"\"\n",
    "    # Lags para features de mercado\n",
    "    for column in columnas:\n",
    "        if column in ['date', 'price']:\n",
    "            continue\n",
    "        df = df.with_columns(pl.col(column).shift(1).alias(f'{column}_lag_1'))\n",
    "\n",
    "    # **CALCULAR _change_1d\n",
    "    for column in columnas:\n",
    "        if column in ['date', 'total_volume']:\n",
    "            continue\n",
    "        df = df.with_columns(((pl.col(column) - pl.col(column).shift(1)) / pl.col(column).shift(1)).alias(f\"{column}_change_1d\")  )     \n",
    "\n",
    "    # Ratios intermercado\n",
    "    for column in columnas:\n",
    "        if column in ['date', 'price', 'total_volume']:\n",
    "            continue\n",
    "        df = df.with_columns((pl.col(\"price\") / pl.col(column)).alias(f\"btc_{column}_ratio\"))\n",
    "\n",
    "    df = df.with_columns([\n",
    "        pl.when(pl.col(\"price_change_1d\") > 0)\n",
    "          .then(1)\n",
    "          .when(pl.col(\"price_change_1d\") < 0)\n",
    "          .then(-1)\n",
    "          .otherwise(0)\n",
    "          .alias(\"price_direction_1d\"),\n",
    "    ])\n",
    "    # Features temporales bÃ¡sicas\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"date\").dt.year().alias(\"year\"),\n",
    "        pl.col(\"date\").dt.month().alias(\"month\"),\n",
    "        pl.col(\"date\").dt.day().alias(\"day\"),\n",
    "        pl.col(\"date\").dt.weekday().alias(\"weekday\"),\n",
    "    ])\n",
    "    # Lags de precio Variados\n",
    "    lags = [15, 30, 45, 60, 75, 90]\n",
    "    for lag in lags:\n",
    "        df = df.with_columns([\n",
    "            # Cambios en lags propuestos\n",
    "            pl.col(\"price\").shift(lag).alias(f\"btc_lag_{lag}\"),\n",
    "        ])\n",
    "    # Rolling statistics de precio\n",
    "    windows = [3, 7, 14, 21, 30]\n",
    "    for window in windows:\n",
    "        df = df.with_columns([\n",
    "            pl.col(\"price\").rolling_std(window).alias(f\"btc_std_{window}\"),\n",
    "        ])\n",
    "    windows = [30, 60, 90]\n",
    "    for window in windows:\n",
    "        df = df.with_columns([\n",
    "            # Medias de los Instrumentos en Ciertas Ventanas\n",
    "            pl.col(\"price\").rolling_mean(window).alias(f\"price_ma_{window}\"),\n",
    "            # Momentum adicional basado en precio (no confundir con price_change_1d)\n",
    "            ((pl.col(\"price\") / pl.col(f\"btc_lag_{window}\")) - 1).alias(f\"btc_momentum_{window}d\"),\n",
    "        ])\n",
    "    # **TARGET: precio de maÃ±ana (SIEMPRE AL FINAL)**\n",
    "    df = df.with_columns(pl.col(\"price\").shift(-1).alias(\"price_tomorrow\"))\n",
    "    df = df.with_columns(\n",
    "        pl.when((pl.col('price_tomorrow') - pl.col('price')) > 0)\n",
    "        .then(1)\n",
    "        .when((pl.col('price_tomorrow') - pl.col('price')) < 0)\n",
    "        .then(-1)\n",
    "        .otherwise(0)\n",
    "        .alias('target_direction')\n",
    "    )\n",
    "    # Eliminar NaNs (ÃšLTIMO PASO)\n",
    "    max_offset = max(max(lags, default=0), max(windows, default=0), 1)\n",
    "    return (df.slice(max_offset, df.shape[0] - max_offset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f03e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_split(df: pl.DataFrame, target: str, test_size: float = 0.4, fecha_corte: date = date(2024, 12, 31)) \\\n",
    "        -> tuple[pl.DataFrame, pl.DataFrame, np.array, np.array, np.array, np.array, pl.DataFrame, pl.DataFrame, np.array, np.array]:\n",
    "    \"\"\"\"\"\"\n",
    "    # DivisiÃ³n\n",
    "    df_trainval = df.filter(pl.col(\"date\") <= fecha_corte)\n",
    "    df_future = df.filter(pl.col(\"date\") > fecha_corte)\n",
    "\n",
    "    # Ordenar por fecha\n",
    "    df_trainval = df_trainval.sort(\"date\")\n",
    "    df_future = df_future.sort(\"date\")\n",
    "    # Split temporal (no aleatorio)\n",
    "    split_idx = int(len(df_trainval) * (1 - test_size))\n",
    "\n",
    "    # Obtengo los DataFrames de Train y Test (dentro de df_trainval que es la Ãºnica data que podrÃ¡ tener contacto con el Modelo)\n",
    "    df_train = df_trainval.slice(0, split_idx)\n",
    "    df_test = df_trainval.slice(split_idx, len(df_trainval) - split_idx)\n",
    "\n",
    "    # Columnas predictoras (excluimos 'date', 'price_tomorrow', 'price_direction')\n",
    "    feature_cols = [col for col in df_train.columns if col not in [\"date\", target, 'price_tomorrow']]\n",
    "\n",
    "    # Convertir a numpy\n",
    "    X_train = df_train.select(feature_cols).to_numpy()\n",
    "    y_train = df_train.select(target).to_numpy().flatten()\n",
    "\n",
    "    X_test = df_test.select(feature_cols).to_numpy()\n",
    "    y_test = df_test.select(target).to_numpy().flatten()\n",
    "\n",
    "    X_test_future = df_future.select(feature_cols).to_numpy()\n",
    "    y_test_future = df_future.select(target).to_numpy().flatten()\n",
    "\n",
    "    return df_trainval, df_future, X_train, y_train, X_test, y_test, df_test, df_train, X_test_future, y_test_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43cf3ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_modelo_clasificacion(preds, y_true, df_test, transaction_cost_pct=0.001) -> dict:\n",
    "    df_test = df_test.sort('date')\n",
    "    prices_hoy = df_test['price'].to_numpy()\n",
    "    prices_tomorrow = df_test['price_tomorrow'].to_numpy()\n",
    "\n",
    "    aciertos_direccion = 0\n",
    "    retornos_porcentuales = []\n",
    "    tasas_libre_riesgos = []\n",
    "\n",
    "    for i in range(len(preds)):\n",
    "        retorno = abs(prices_tomorrow[i] - prices_hoy[i])\n",
    "        tasa_libre = prices_hoy[i] * transaction_cost_pct\n",
    "        tasas_libre_riesgos.append(tasa_libre)\n",
    "\n",
    "        if preds[i] == y_true[i]:\n",
    "            aciertos_direccion += 1\n",
    "            retornos_porcentuales.append((retorno - tasa_libre) / prices_hoy[i])\n",
    "        else:\n",
    "            retornos_porcentuales.append(-(retorno + tasa_libre) / prices_hoy[i])\n",
    "\n",
    "    exce_retorno = np.array(retornos_porcentuales) - np.array(tasas_libre_riesgos)\n",
    "    sharpe_ratio = np.mean(exce_retorno) / np.std(exce_retorno)\n",
    "    directional_accuracy = aciertos_direccion / len(preds)\n",
    "    cumulative_return = np.prod(1 + np.array(retornos_porcentuales)) - 1\n",
    "    cagr = (1 + cumulative_return)**(365 / len(preds)) - 1\n",
    "\n",
    "    return {\n",
    "        \"directional_accuracy\": directional_accuracy,\n",
    "        \"sharpe_ratio\": sharpe_ratio,\n",
    "        \"cumulative_return\": cumulative_return,\n",
    "        \"Compound_Annual_Growth_Rate\": cagr,\n",
    "        \"n_evaluated_days\": len(preds),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df44c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrena_evalua_lightgbm(params: dict,\n",
    "                            X_train: np.array,\n",
    "                            y_train: np.array,\n",
    "                            X_test: np.array,\n",
    "                            y_test: np.array,\n",
    "                            df_test: pl.DataFrame,\n",
    "                            X_test_future: np.array,\n",
    "                            y_test_future: np.array,\n",
    "                            df_future: pl.DataFrame,\n",
    "                            transaction_cost_pct: float = 0.001,\n",
    "                            iteracion: int = 0,\n",
    "                            return_model: bool = False) -> tuple[dict, dict]:\n",
    "\n",
    "    model = LGBMClassifier(random_state=42, **params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    if return_model:\n",
    "        return model\n",
    "\n",
    "    preds_test = model.predict(X_test)\n",
    "    preds_future = model.predict(X_test_future)\n",
    "\n",
    "    metricas_test = evaluar_modelo_clasificacion(preds_test, y_test, df_test, transaction_cost_pct)\n",
    "    metricas_future = evaluar_modelo_clasificacion(preds_future, y_test_future, df_future, transaction_cost_pct)\n",
    "\n",
    "    iteraciones_dict = {\"iteracion\": iteracion}\n",
    "\n",
    "    return metricas_test | params | iteraciones_dict, metricas_future | params | iteraciones_dict, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d71228c",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## Empieza el Flujo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02fbf99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(\"db/db.parquet\")\n",
    "target = 'target_direction'\n",
    "\n",
    "# ['date', 'price', 'total_volume', 'market_cap', 'price_gold', 'stock_index_dowjones', 'stock_index_sp500', 'rate_US10Y', 'stock_index_ni225']\n",
    "columns = ['date', 'price', 'total_volume', 'stock_index_sp500', 'price_gold', 'rate_US10Y', 'stock_index_ni225']\n",
    "df = df.select(columns)\n",
    "\n",
    "# AÃ‘ADO MULTIPLES FEATURES CREADAS A PARTIR DE LOS PRECIOS DE LOS DISTINTOS INSTRUMENTOS Y SUS RELACIONES\n",
    "df = df.pipe(add_features, columns)\n",
    "\n",
    "# DESCOMPOSICION DE LOS DATOS EN SET DE DATOS PARA ENTRENAR, TESTEAR Y LUEGO PROBRA CON OTRO SET (PARA PROBAR CON DATOS QUE EL MODELO NUNCA HAYA VISTO)\n",
    "df_trainval, df_future, X_train, y_train, X_test, y_test, df_test, df_train, X_test_future, y_test_future = df.pipe(temporal_split, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe443384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probando 120 combinaciones...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b713327e3f404c9a9af473fd4b79ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entrenando modelos:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"d:\\01_practica\\pro-bitcoin\\modelos\\venv\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 247, in _count_physical_cores\n",
      "    cpu_count_physical = _count_physical_cores_win32()\n",
      "  File \"d:\\01_practica\\pro-bitcoin\\modelos\\venv\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 299, in _count_physical_cores_win32\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"C:\\Users\\alisk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py\", line 503, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"C:\\Users\\alisk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py\", line 971, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\alisk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py\", line 1456, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [200, 300, 400, 500, 600]\n",
    "max_depth = [2, 3, 4]\n",
    "learning_rates = [0.1, 0.2]\n",
    "subsamples = [0.8, 0.9]\n",
    "colsamples_bytree = [0.8, 0.9]\n",
    "\n",
    "\n",
    "# Calcular total de combinaciones\n",
    "total_combinaciones = len(n_estimators) * len(max_depth) * len(learning_rates) * len(subsamples) * len(colsamples_bytree)\n",
    "\n",
    "print(f\"Probando {total_combinaciones} combinaciones...\")\n",
    "\n",
    "data_test = []\n",
    "data_real_2025 = []\n",
    "modelos = []\n",
    "combinaciones = list(itertools.product(n_estimators, max_depth, learning_rates, subsamples, colsamples_bytree))\n",
    "\n",
    "iteracion = 0\n",
    "for estimator, depth, lr, subsample, colsample in tqdm(combinaciones, desc=\"Entrenando modelos\"):\n",
    "    params = {\n",
    "        \"n_estimators\": estimator,\n",
    "        \"max_depth\": depth,\n",
    "        \"learning_rate\": lr,\n",
    "        \"subsample\": subsample,\n",
    "        \"colsample_bytree\": colsample,\n",
    "        \"verbose\": -1,           # Silencia logs\n",
    "        \"verbosity\": -1,         # Silencia logs (redundante pero seguro)\n",
    "        \"log_level\": \"fatal\"     # Solo muestra errores crÃ­ticos\n",
    "    }\n",
    "\n",
    "    datos_dict_test, datos_dict_real, modelo = entrena_evalua_lightgbm(\n",
    "        params=params,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        df_test=df_test,\n",
    "        X_test_future=X_test_future,\n",
    "        y_test_future=y_test_future,\n",
    "        df_future=df_future,\n",
    "        iteracion=iteracion,\n",
    "    )\n",
    "\n",
    "    data_test.append(datos_dict_test)\n",
    "    data_real_2025.append(datos_dict_real)\n",
    "    modelos.append(modelo)\n",
    "    iteracion += 1\n",
    "\n",
    "df_parametros_data_test = pl.DataFrame(data_test)\n",
    "df_parametros_data_2025 = pl.DataFrame(data_real_2025)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ad05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndirectional_accuracy    ########   y   ########   con [price, total_volume, stock_index_sp500, price_gold, rate_US10Y, stock_index_dowjones, stock_index_ni225]\\n\\ndirectional_accuracy    ########   y   ########   con [price, total_volume, stock_index_sp500, price_gold, rate_US10Y, stock_index_ni225]      *** SELECCIONADOS\\ndirectional_accuracy    ########   y   ########   con [price, total_volume, stock_index_sp500, price_gold, rate_US10Y, stock_index_dowjones]\\n\\ndirectional_accuracy    ########   y   ########   con [price, total_volume, stock_index_sp500, price_gold, rate_US10Y]\\n\\ndirectional_accuracy    ########   y   0.543796   con [price, rate_US10Y, price_gold, total_volume]\\n\\ndirectional_accuracy    ########   y   0.547445   con [price, rate_US10Y, price_gold]\\n\\ndirectional_accuracy    ########   y   0.529197   con [price, total_volume, stock_index_sp500]\\n\\ndirectional_accuracy    ########   y   0.50365   con [price, stock_index_sp500]\\n\\ndirectional_accuracy    ########   y   0.532847   con [price, total_volume]\\n\\ndirectional_accuracy    ########   y   0.521898   con [price]\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "directional_accuracy   0.525547   con [price, total_volume, stock_index_sp500, price_gold, rate_US10Y, stock_index_ni225]   \n",
    "\n",
    "directional_accuracy   0.543796   con [price, rate_US10Y, price_gold, total_volume]\n",
    "\n",
    "directional_accuracy   0.547445   con [price, rate_US10Y, price_gold]\n",
    "\n",
    "directional_accuracy   0.529197   con [price, total_volume, stock_index_sp500]\n",
    "\n",
    "directional_accuracy   0.50365   con [price, stock_index_sp500]\n",
    "\n",
    "directional_accuracy   0.532847   con [price, total_volume]\n",
    "\n",
    "directional_accuracy   0.521898   con [price]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17e0d2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 14)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>directional_accuracy</th><th>sharpe_ratio</th><th>cumulative_return</th><th>Compound_Annual_Growth_Rate</th><th>n_evaluated_days</th><th>n_estimators</th><th>max_depth</th><th>learning_rate</th><th>subsample</th><th>colsample_bytree</th><th>verbose</th><th>verbosity</th><th>log_level</th><th>iteracion</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>0.525547</td><td>NaN</td><td>NaN</td><td>NaN</td><td>274</td><td>300</td><td>4</td><td>0.2</td><td>0.8</td><td>0.9</td><td>-1</td><td>-1</td><td>&quot;fatal&quot;</td><td>45</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 14)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ directiona â”† sharpe_ra â”† cumulativ â”† Compound_ â”† â€¦ â”† verbose â”† verbosity â”† log_level â”† iteracion â”‚\n",
       "â”‚ l_accuracy â”† tio       â”† e_return  â”† Annual_Gr â”†   â”† ---     â”† ---       â”† ---       â”† ---       â”‚\n",
       "â”‚ ---        â”† ---       â”† ---       â”† owth_Rate â”†   â”† i64     â”† i64       â”† str       â”† i64       â”‚\n",
       "â”‚ f64        â”† f64       â”† f64       â”† ---       â”†   â”†         â”†           â”†           â”†           â”‚\n",
       "â”‚            â”†           â”†           â”† f64       â”†   â”†         â”†           â”†           â”†           â”‚\n",
       "â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 0.525547   â”† NaN       â”† NaN       â”† NaN       â”† â€¦ â”† -1      â”† -1        â”† fatal     â”† 45        â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parametros_data_2025.sort(\"directional_accuracy\", descending=True).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8155a1d",
   "metadata": {},
   "source": [
    "### Recuperando el Modelo con el mejor `directional_accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3cc4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros_data_2025 = df_parametros_data_2025.sort(\"directional_accuracy\", descending=True).head(1).to_dicts()[0]\n",
    "modelo_comprobacion = modelos[parametros_data_2025['iteracion']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36641fd",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### ComprobaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e4b049d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directional_accuracy --> 0.5255474452554745\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def comprobacion(model):\n",
    "\n",
    "    # Columnas Features\n",
    "    features_pytorch = [col for col in df_future.columns if col not in [\"date\", 'target_direction', 'price_tomorrow']]\n",
    "\n",
    "    # Modelo tal cual sera utilizado en Produccion\n",
    "    xgboost_model = model\n",
    " \n",
    "    X_input_xgboost = X_test_future\n",
    "    \n",
    "    # Inferencia \n",
    "    preds = model.predict(X_input_xgboost)\n",
    "    y_test = copy.deepcopy(y_test_future)\n",
    "\n",
    "    aciertos_direccion = 0\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] == y_test[i]:\n",
    "            aciertos_direccion += 1\n",
    "  \n",
    "    directional_accuracy = aciertos_direccion / len(preds)\n",
    "    return directional_accuracy\n",
    "\n",
    "\n",
    "directional_accuracy = comprobacion(modelo_comprobacion)\n",
    "print(f\"directional_accuracy --> {directional_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14527980",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### ðŸ’¾ Guardando los Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8ecfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(modelo_comprobacion, \"models/lightgbm_model_data_2025.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
