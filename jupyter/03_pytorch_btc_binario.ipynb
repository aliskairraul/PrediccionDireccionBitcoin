{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eec7df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Tus importaciones originales se mantienen\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5736e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"Fija la semilla para todas las librerÃ­as relevantes para la reproducibilidad.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "        \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb44dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df: pl.DataFrame, columnas: list) -> pl.DataFrame:        \n",
    "    \"\"\"... (cÃ³digo original sin cambios) ...\"\"\"\n",
    "    # Lags para features de mercado\n",
    "    for column in columnas:\n",
    "        if column in ['date', 'price']:\n",
    "            continue\n",
    "        df = df.with_columns(pl.col(column).shift(1).alias(f'{column}_lag_1'))\n",
    "\n",
    "    # **CALCULAR _change_1d\n",
    "    for column in columnas:\n",
    "        if column in ['date', 'total_volume']:\n",
    "            continue\n",
    "        df = df.with_columns(((pl.col(column) - pl.col(column).shift(1)) / pl.col(column).shift(1)).alias(f\"{column}_change_1d\")  )     \n",
    "\n",
    "    # Ratios intermercado\n",
    "    for column in columnas:\n",
    "        if column in ['date', 'price', 'total_volume']:\n",
    "            continue\n",
    "        df = df.with_columns((pl.col(\"price\") / pl.col(column)).alias(f\"btc_{column}_ratio\"))\n",
    "\n",
    "    df = df.with_columns([\n",
    "        pl.when(pl.col(\"price_change_1d\") > 0)\n",
    "          .then(1)\n",
    "          .when(pl.col(\"price_change_1d\") < 0)\n",
    "          .then(-1)\n",
    "          .otherwise(0)\n",
    "          .alias(\"price_direction_1d\"),\n",
    "    ])\n",
    "    # Features temporales bÃ¡sicas\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"date\").dt.year().alias(\"year\"),\n",
    "        pl.col(\"date\").dt.month().alias(\"month\"),\n",
    "        pl.col(\"date\").dt.day().alias(\"day\"),\n",
    "        pl.col(\"date\").dt.weekday().alias(\"weekday\"),\n",
    "    ])\n",
    "    # Lags de precio Variados\n",
    "    lags = [15, 30, 45, 60, 75, 90]\n",
    "    for lag in lags:\n",
    "        df = df.with_columns([\n",
    "            # Cambios en lags propuestos\n",
    "            pl.col(\"price\").shift(lag).alias(f\"btc_lag_{lag}\"),\n",
    "        ])\n",
    "    # Rolling statistics de precio\n",
    "    windows = [3, 7, 14, 21, 30]\n",
    "    for window in windows:\n",
    "        df = df.with_columns([\n",
    "            pl.col(\"price\").rolling_std(window).alias(f\"btc_std_{window}\"),\n",
    "        ])\n",
    "    windows = [30, 60, 90]\n",
    "    for window in windows:\n",
    "        df = df.with_columns([\n",
    "            # Medias de los Instrumentos en Ciertas Ventanas\n",
    "            pl.col(\"price\").rolling_mean(window).alias(f\"price_ma_{window}\"),\n",
    "            # Momentum adicional basado en precio (no confundir con price_change_1d)\n",
    "            ((pl.col(\"price\") / pl.col(f\"btc_lag_{window}\")) - 1).alias(f\"btc_momentum_{window}d\"),\n",
    "        ])\n",
    "    # **TARGET: precio de maÃ±ana (SIEMPRE AL FINAL)**\n",
    "    df = df.with_columns(pl.col(\"price\").shift(-1).alias(\"price_tomorrow\"))\n",
    "    df = df.with_columns(\n",
    "        pl.when((pl.col('price_tomorrow') - pl.col('price')) > 0)\n",
    "        .then(1)\n",
    "        .when((pl.col('price_tomorrow') - pl.col('price')) < 0)\n",
    "        .then(-1)\n",
    "        .otherwise(0)\n",
    "        .alias('target_direction')\n",
    "    )\n",
    "    # Eliminar NaNs (ÃšLTIMO PASO)\n",
    "    max_offset = max(max(lags, default=0), max(windows, default=0), 1)\n",
    "    return (df.slice(max_offset, df.shape[0] - max_offset)).drop_nulls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98b35e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_split(df: pl.DataFrame, target: str, test_size: float = 0.4, fecha_corte: date = date(2024, 12, 31)):\n",
    "    # ... (Esta funciÃ³n tambiÃ©n se mantiene casi igual, solo se aÃ±ade el escalado de datos)\n",
    "    \"\"\"... (cÃ³digo original con la adiciÃ³n del scaler) ...\"\"\"\n",
    "    df_trainval = df.filter(pl.col(\"date\") <= fecha_corte)\n",
    "    df_future = df.filter(pl.col(\"date\") > fecha_corte)\n",
    "    df_trainval = df_trainval.sort(\"date\")\n",
    "    df_future = df_future.sort(\"date\")\n",
    "    split_idx = int(len(df_trainval) * (1 - test_size))\n",
    "    df_train = df_trainval.slice(0, split_idx)\n",
    "    df_test = df_trainval.slice(split_idx, len(df_trainval) - split_idx)\n",
    "    feature_cols = [col for col in df_train.columns if col not in [\"date\", target, 'price_tomorrow']]\n",
    "\n",
    "    # --- Â¡NUEVO Y MUY IMPORTANTE PARA REDES NEURONALES! ---\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(df_train.select(feature_cols).to_numpy())\n",
    "    X_test = scaler.transform(df_test.select(feature_cols).to_numpy())\n",
    "    X_test_future = scaler.transform(df_future.select(feature_cols).to_numpy())\n",
    "    # --- FIN DE LA SECCIÃ“N NUEVA ---\n",
    "\n",
    "    y_train = df_train.select(target).to_numpy().flatten()\n",
    "    y_test = df_test.select(target).to_numpy().flatten()\n",
    "    y_test_future = df_future.select(target).to_numpy().flatten()\n",
    "    return df_trainval, df_future, X_train, y_train, X_test, y_test, df_test, df_train, X_test_future, y_test_future, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf64c1a",
   "metadata": {},
   "source": [
    "## DefiniciÃ³n del Modelo Pytorch\n",
    "#### `MLP (Multi-Layer Perceptron)`\n",
    "- **Capa Lineal (nn.Linear):**\n",
    "    - Realiza una transformaciÃ³n lineal (y=Wx+b). Es la capa que \"aprende\" las relaciones entre las features.\n",
    "- **ActivaciÃ³n (nn.ReLU):** \n",
    "    - Introduce la no-linealidad. Sin esto, apilar capas lineales serÃ­a matemÃ¡ticamente igual a tener una sola capa lineal, y el modelo no podrÃ­a aprender patrones complejos. ReLU simplemente convierte todos los valores negativos en cero.\n",
    "- **RegularizaciÃ³n (nn.Dropout):** \n",
    "    - Como vimos, combate el sobreajuste.\n",
    "- **Capa de Salida:** \n",
    "    - Es una capa lineal con una sola neurona para producir un Ãºnico valor de salida (nuestra predicciÃ³n de target_direction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e20c6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitcoinDirectionNet(nn.Module):\n",
    "    \"\"\"Red neuronal optimizada para predecir direcciÃ³n del precio de Bitcoin.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims=[256, 128, 64], dropout_prob=0.3):\n",
    "        super(BitcoinDirectionNet, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # ConstrucciÃ³n dinÃ¡mica de capas\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),  # BatchNorm para mejor estabilidad\n",
    "                nn.LeakyReLU(0.1),  # LeakyReLU para evitar neuronas muertas\n",
    "                nn.Dropout(dropout_prob)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Capa final de clasificaciÃ³n\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        layers.append(nn.Sigmoid())  # Sigmoid para probabilidades [0,1]\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db05be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_modelo_pytorch(\n",
    "    modelo: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    df_test: pl.DataFrame,\n",
    "    device: torch.device,\n",
    "    transaction_cost_pct: float = 0.001,\n",
    "    threshold: float = 0.5  # Umbral para clasificaciÃ³n\n",
    ") -> dict:\n",
    "    \"\"\"FunciÃ³n de evaluaciÃ³n adaptada para PyTorch.\"\"\"\n",
    "    modelo.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in data_loader:\n",
    "            features = features.to(device)\n",
    "            outputs = modelo(features)\n",
    "            probs = outputs.cpu().numpy().flatten()\n",
    "            preds = (probs > threshold).astype(int) * 2 - 1  # Convertir a -1/1\n",
    "            all_preds.extend(preds)\n",
    "            all_probs.extend(probs)\n",
    "\n",
    "    preds = np.array(all_preds)\n",
    "\n",
    "    df_test = df_test.sort('date')\n",
    "    prices_hoy = df_test['price'].to_numpy()\n",
    "    prices_tomorrow = df_test['price_tomorrow'].to_numpy()\n",
    "    y_test = df_test['target_direction'].to_numpy()  # Obtenemos y_test del df\n",
    "\n",
    "    # El resto de la lÃ³gica de cÃ¡lculo de mÃ©tricas es idÃ©ntica\n",
    "    aciertos_direccion = 0\n",
    "    retornos = []\n",
    "    retornos_porcentuales = []\n",
    "    tasas_libre_riesgos = []\n",
    "    for i in range(len(preds)):\n",
    "        retorno = abs(prices_tomorrow[i] - prices_hoy[i])\n",
    "        tasa_libre = prices_hoy[i] * transaction_cost_pct\n",
    "        tasas_libre_riesgos.append(tasa_libre)\n",
    "        if (preds[i] > 0.5 and y_test[i] == 1) or (preds[i] < 0.5 and y_test[i] == -1):\n",
    "            aciertos_direccion += 1\n",
    "            retornos.append(retorno)\n",
    "            retornos_porcentuales.append((retorno - tasa_libre) / prices_hoy[i])\n",
    "        else:\n",
    "            retornos.append(-retorno)\n",
    "            retornos_porcentuales.append(-(retorno + tasa_libre) / prices_hoy[i])\n",
    "\n",
    "    exce_retorno = np.array(retornos) - np.array(tasas_libre_riesgos)\n",
    "    sharpe_ratio = np.mean(exce_retorno) / np.std(exce_retorno) if np.std(exce_retorno) != 0 else 0\n",
    "    directional_accuracy = aciertos_direccion / len(preds)\n",
    "    n_evaluated_days = len(preds)\n",
    "    retornos_porcentuales_np = np.array(retornos_porcentuales)\n",
    "    cumulative_return = np.prod(1 + retornos_porcentuales_np) - 1\n",
    "    cagr = (1 + cumulative_return)**(365 / n_evaluated_days) - 1\n",
    "\n",
    "    return {\n",
    "        \"directional_accuracy\": directional_accuracy,\n",
    "        \"sharpe_ratio\": sharpe_ratio,\n",
    "        \"cumulative_return\": cumulative_return,\n",
    "        \"Compound_Annual_Growth_Rate\": cagr,\n",
    "        \"n_evaluated_days\": n_evaluated_days,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cbea4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrena_evalua_pytorch(\n",
    "    hidden_dim: int,\n",
    "    learning_rate: float,\n",
    "    dropout: float,\n",
    "    epochs: int,\n",
    "    batch_size: int,\n",
    "    train_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    future_loader: DataLoader,\n",
    "    df_test: pl.DataFrame,\n",
    "    df_future: pl.DataFrame,\n",
    "    device: torch.device,\n",
    "    iteracion: int = 0,\n",
    "    return_model: bool=False\n",
    ") -> dict:\n",
    "\n",
    "    input_dim = next(iter(train_loader))[0].shape[1]\n",
    "\n",
    "    # Definir la arquitectura exacta\n",
    "    hidden_dims = [hidden_dim, hidden_dim//2, hidden_dim//4]\n",
    "\n",
    "    # 1. Instanciar modelo, funciÃ³n de pÃ©rdida y optimizador\n",
    "    # model = BitcoinDirectionNet(input_dim, [hidden_dim, hidden_dim//2, hidden_dim//4], dropout).to(device)\n",
    "    model = BitcoinDirectionNet(input_dim, hidden_dims, dropout).to(device)\n",
    "    \n",
    "    # Cambiar a pÃ©rdida de clasificaciÃ³n binaria\n",
    "    criterion = nn.BCELoss()  # Binary Cross Entropy para clasificaciÃ³n\n",
    "    \n",
    "    # Optimizador con weight decay (regularizaciÃ³n L2)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "    # Scheduler para ajustar learning rate\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)\n",
    "\n",
    "    # --- INICIALIZACIÃ“N DE EARLY STOPPING ---\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    # -----------------------------------------\n",
    "    data_epoch_loss = []\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Bucle de entrenamiento\n",
    "        model.train()  # Poner el modelo en modo de entrenamiento\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "        running_train_loss = 0.0 \n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            # Convertir labels a formato binario [0,1]\n",
    "            # Asumiendo que labels son -1/1, convertimos a 0/1\n",
    "            binary_labels = (labels > 0).float().unsqueeze(1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, binary_labels)\n",
    "\n",
    "            # Backward pass y optimizaciÃ³n\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping para estabilidad\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "        # ------------------------  ELIMINAR ESTE BLOQUE SI VA EL DE ABAJO    \n",
    "        avg_epoch_loss = epoch_loss / batch_count\n",
    "        scheduler.step(avg_epoch_loss)\n",
    "\n",
    "\n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        diccionario_epoch_loss = {}\n",
    "        diccionario_epoch_loss['epoch'] = epoch\n",
    "        diccionario_epoch_loss['train_loss'] = round(avg_train_loss, 4)\n",
    "        diccionario_epoch_loss['lr'] = optimizer.param_groups[0]['lr']\n",
    "        data_epoch_loss.append(diccionario_epoch_loss)\n",
    "        # -----------------------------------------------------------------\n",
    "\n",
    "    model.eval()\n",
    "    if return_model:\n",
    "        return model, optimizer\n",
    "    \n",
    "    # 3. EvaluaciÃ³n\n",
    "    metricas_test = evaluar_modelo_pytorch(model, test_loader, df_test, device)\n",
    "    metricas_future = evaluar_modelo_pytorch(model, future_loader, df_future, device)\n",
    "\n",
    "    params = {\n",
    "        'input_dim': input_dim,\n",
    "        'hidden_dims': hidden_dims,\n",
    "        'learning_rate': learning_rate,\n",
    "        'dropout': dropout, \n",
    "        'epochs': epochs, \n",
    "        'batch_size': batch_size,\n",
    "        'iteracion': iteracion\n",
    "    }\n",
    "\n",
    "    epochs_loss = {\n",
    "        'data_epochs_loss': data_epoch_loss\n",
    "    }\n",
    "\n",
    "    return metricas_test | params, metricas_future | params, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49437995",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## Empieza el Flujo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42aeecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "df = pl.read_parquet(\"db/db.parquet\")\n",
    "target = 'target_direction'\n",
    "# ['date', 'price', 'total_volume', 'market_cap', 'price_gold', 'stock_index_dowjones', 'stock_index_sp500', 'rate_US10Y', 'stock_index_ni225']\n",
    "columns = ['date', 'price', 'total_volume', 'stock_index_sp500', 'price_gold', 'rate_US10Y', 'stock_index_ni225']\n",
    "df = df.select(columns)\n",
    "df = df.pipe(add_features, columns)\n",
    "\n",
    "df_trainval, df_future, X_train, y_train, X_test, y_test, df_test, df_train, X_test_future, y_test_future, scaler_global = df.pipe(temporal_split, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb56ecc9",
   "metadata": {},
   "source": [
    "### Determinar Dispositivo  (GPU si estÃ¡ disponible)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a55399a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130cc884",
   "metadata": {},
   "source": [
    "#### `hidden_dims (TamaÃ±o de las Capas Ocultas)`\n",
    "- **TeorÃ­a:**\n",
    "    - Es el \"poder cerebral\" de tu red. Define cuÃ¡ntas neuronas hay en cada capa oculta. MÃ¡s neuronas significan que la red tiene mÃ¡s capacidad para aprender patrones complejos.\n",
    "- **Recomendaciones:**\n",
    "    - No hay una regla de oro, pero para datos tabulares, empezar con valores entre 32 y 256 es comÃºn.\n",
    "    - Una arquitectura popular es la de \"embudo\", donde cada capa sucesiva es mÃ¡s pequeÃ±a (ej., [256, 128, 64]).\n",
    "    - El tamaÃ±o de la capa de entrada (el nÃºmero de features) puede ser una buena guÃ­a para el tamaÃ±o de la primera capa oculta.\n",
    "- **Riesgos:**\n",
    "    - Valores muy altos (ej., > 512): Alto riesgo de sobreajuste. La red tiene tanta capacidad que puede memorizar cada dato de entrenamiento, incluido el ruido.\n",
    "    - Valores muy bajos (ej., < 32): Riesgo de subajuste (underfitting). La red no tiene suficiente \"poder\" para capturar la complejidad de los datos.\n",
    "\n",
    "#### `learning_rates (Tasa de Aprendizaje)`\n",
    "- **TeorÃ­a:** \n",
    "    - Es el tamaÃ±o del paso que da el optimizador al ajustar los pesos del modelo. Es quizÃ¡s el hiperparÃ¡metro mÃ¡s importante.\n",
    "    - Imagina que estÃ¡s bajando una montaÃ±a a ciegas. La tasa de aprendizaje es el tamaÃ±o de tus pasos.\n",
    "- **Recomendaciones:**\n",
    "    - Es un parÃ¡metro muy sensible. Los valores mÃ¡s comunes se mueven en un rango logarÃ­tmico: 0.01, 0.005, 0.001, 0.0005, 0.0001.\n",
    "    - Empezar con 0.001 es casi siempre una apuesta segura.\n",
    "- **Riesgos:**\n",
    "    - Valor muy alto (ej., > 0.01): Divergencia. Los pasos son tan grandes que el modelo \"salta\" por encima de la soluciÃ³n Ã³ptima una y otra vez, y la pÃ©rdida (el error) puede explotar en lugar de disminuir.\n",
    "    - Valor muy bajo (ej., < 0.0001): Convergencia muy lenta. El entrenamiento tomarÃ¡ una eternidad y el modelo podrÃ­a quedarse atascado en una mala soluciÃ³n (mÃ­nimo local) porque sus pasos son demasiado tÃ­midos para salir de ahÃ­.\n",
    "\n",
    "#### `dropouts (Tasa de Abandono)`\n",
    "- **TeorÃ­a:**\n",
    "    - Es una tÃ©cnica de regularizaciÃ³n fundamental para combatir el sobreajuste. En cada paso de entrenamiento, \"apaga\" aleatoriamente un porcentaje de neuronas. Esto fuerza a la red a aprender de forma mÃ¡s robusta, sin depender excesivamente de unas pocas neuronas.\n",
    "- **Recomendaciones:**\n",
    "    - Valores comunes estÃ¡n entre 0.1 y 0.5.\n",
    "    - Un valor de 0.5 es agresivo pero muy efectivo. Un valor de 0.2 o 0.3 es un punto de partida mÃ¡s conservador.\n",
    "- **Riesgos:**\n",
    "    - Valor muy alto (ej., > 0.6): Riesgo de subajuste. EstÃ¡s apagando tantas neuronas que la red no tiene suficiente capacidad para aprender adecuadamente durante el entrenamiento.\n",
    "    - Valor muy bajo (ej., < 0.1): El efecto de regularizaciÃ³n es mÃ­nimo, por lo que el riesgo de sobreajuste sigue siendo alto.\n",
    "\n",
    "#### `epochs_list (NÃºmero de Ã‰pocas)`\n",
    "- **TeorÃ­a:** \n",
    "    - Una Ã©poca es una pasada completa del modelo por todo el conjunto de datos de entrenamiento. Es la cantidad de veces que el modelo \"estudia\" los datos.\n",
    "- **Recomendaciones:**\n",
    "    - Esto depende mucho del tamaÃ±o del dataset y la complejidad del problema. Puede variar desde 20 hasta 200+.\n",
    "- **PrÃ¡ctica recomendada:**\n",
    "    - En lugar de fijar un nÃºmero de Ã©pocas, se usa una tÃ©cnica llamada \"Early Stopping\" (DetenciÃ³n Temprana). Monitoreas el rendimiento en un set de validaciÃ³n y detienes el entrenamiento cuando el rendimiento en ese set deja de mejorar, evitando asÃ­ el sobreajuste.\n",
    "- **Riesgos:**\n",
    "    - Demasiadas Ã©pocas: Causa principal de sobreajuste. El modelo aprende perfectamente los datos de entrenamiento pero pierde su capacidad de generalizar a datos nuevos.\n",
    "    - Muy pocas Ã©pocas: Subajuste. El modelo no ha tenido tiempo suficiente para aprender los patrones.\n",
    "\n",
    "#### `batch_size (TamaÃ±o del Lote)`\n",
    "- **TeorÃ­a:**\n",
    "    - Es el nÃºmero de muestras de datos que el modelo ve antes de actualizar sus pesos.\n",
    "- **Recomendaciones:**\n",
    "    - Se suelen usar potencias de 2 por optimizaciones de hardware: 32, 64, 128, 256.\n",
    "    - 64 o 128 son puntos de partida excelentes.\n",
    "    - Depende de la memoria de tu GPU (VRAM). Si te da un error de \"out of memory\", debes reducir el batch_size.\n",
    "- **Riesgos:**\n",
    "    - Valor muy grande: El entrenamiento es mÃ¡s rÃ¡pido, pero puede generalizar peor, ya que los ajustes de los pesos son menos frecuentes y mÃ¡s \"promediados\".\n",
    "    - Valor muy pequeÃ±o: El entrenamiento es mÃ¡s lento y los ajustes son mÃ¡s \"ruidosos\", lo que a veces puede ayudar a la generalizaciÃ³n (actÃºa como una forma de regularizaciÃ³n), pero puede hacer que la convergencia sea inestable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c6657",
   "metadata": {},
   "source": [
    "### CreaciÃ³n de Tensores y DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0df2f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a tensores de PyTorch\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32)\n",
    "X_future_t = torch.tensor(X_test_future, dtype=torch.float32)\n",
    "y_future_t = torch.tensor(y_test_future, dtype=torch.float32)\n",
    "\n",
    "# Crear Datasets\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "future_dataset = TensorDataset(X_future_t, y_future_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbffaa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probando 72 combinaciones de hiperparÃ¡metros de PyTorch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ebc0c4d9804a3ead80dc15583481b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entrenando modelos PyTorch:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# HiperparÃ¡metros de PyTorch\n",
    "hidden_dims = [64, 128]\n",
    "learning_rates = [0.001, 0.002, 0.003]\n",
    "dropouts = [0.2, 0.3]\n",
    "epochs_list = [40, 50, 60, 70, 80, 90]\n",
    "batch_size = 128  # Fijo para este ejemplo\n",
    "\n",
    "combinaciones = list(itertools.product(hidden_dims, learning_rates, dropouts, epochs_list))\n",
    "print(f\"Probando {len(combinaciones)} combinaciones de hiperparÃ¡metros de PyTorch...\")\n",
    "\n",
    "data_test = []\n",
    "data_2025 = []\n",
    "modelos = []\n",
    "iteracion = 0\n",
    "for hidden, lr, drop, epochs in tqdm(combinaciones, desc=\"Entrenando modelos PyTorch\"):\n",
    "    # Crear DataLoaders para la iteraciÃ³n actual\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    future_loader = DataLoader(future_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    datos_dict_test, datos_dict_real, modelo = entrena_evalua_pytorch(\n",
    "        hidden_dim=hidden, \n",
    "        learning_rate=lr, \n",
    "        dropout=drop, \n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size, \n",
    "        train_loader=train_loader, \n",
    "        test_loader=test_loader,\n",
    "        future_loader=future_loader, \n",
    "        df_test=df_test, \n",
    "        df_future=df_future, \n",
    "        device=device,\n",
    "        iteracion=iteracion\n",
    "    )\n",
    "    data_test.append(datos_dict_test)\n",
    "    data_2025.append(datos_dict_real)\n",
    "    modelos.append(modelo)\n",
    "    \n",
    "    iteracion += 1\n",
    "\n",
    "df_parametros_data_test = pl.DataFrame(data_test)\n",
    "df_parametros_data_2025 = pl.DataFrame(data_2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc9f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndirectional_accuracy    ########   y   0.564103   con [price, total_volume, stock_index_sp500, price_gold, rate_US10Y, stock_index_ni225, stock_index_dowjones]\\n\\ndirectional_accuracy    ########   y   0.582418   con [price, total_volume, stock_index_sp500, price_gold,  rate_US10Y, stock_index_ni225]\\n\\ndirectional_accuracy    ########   y   0.545788   con [price, total_volume, stock_index_sp500, stock_index_ni225]\\n\\ndirectional_accuracy    ########   y   0.534799   con [price, total_volume, stock_index_sp500, price_gold]\\n\\ndirectional_accuracy    ########   y   0.538462   con [price, total_volume, stock_index_sp500, rate_US10Y]\\n\\ndirectional_accuracy    ########   y   0.567766   con [price, total_volume, stock_index_sp500]\\n\\ndirectional_accuracy    ########   y   0.555147   con [price, total_volume]\\n\\ndirectional_accuracy    ########   y   0.540441   con [price]\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "directional_accuracy   0.564103   con [price, total_volume, stock_index_sp500, price_gold, rate_US10Y, stock_index_ni225, stock_index_dowjones]\n",
    "\n",
    "directional_accuracy   0.582418   con [price, total_volume, stock_index_sp500, price_gold,  rate_US10Y, stock_index_ni225]\n",
    "\n",
    "directional_accuracy   0.545788   con [price, total_volume, stock_index_sp500, stock_index_ni225]\n",
    "\n",
    "directional_accuracy   0.534799   con [price, total_volume, stock_index_sp500, price_gold]\n",
    "\n",
    "directional_accuracy   0.538462   con [price, total_volume, stock_index_sp500, rate_US10Y]\n",
    "\n",
    "directional_accuracy   0.567766   con [price, total_volume, stock_index_sp500]\n",
    "\n",
    "directional_accuracy   0.555147   con [price, total_volume]\n",
    "\n",
    "directional_accuracy   0.540441   con [price]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09b0dc3",
   "metadata": {},
   "source": [
    "#### Estos son los parÃ¡metros del Modelo con Mejor `directional_acrruracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f8bf592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>directional_accuracy</th><th>sharpe_ratio</th><th>cumulative_return</th><th>Compound_Annual_Growth_Rate</th><th>n_evaluated_days</th><th>input_dim</th><th>hidden_dims</th><th>learning_rate</th><th>dropout</th><th>epochs</th><th>batch_size</th><th>iteracion</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>list[i64]</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>0.582418</td><td>0.077726</td><td>0.44833</td><td>0.64089</td><td>273</td><td>42</td><td>[64, 32, 16]</td><td>0.003</td><td>0.2</td><td>60</td><td>128</td><td>26</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 12)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ directiona â”† sharpe_rat â”† cumulative â”† Compound_A â”† â€¦ â”† dropout â”† epochs â”† batch_siz â”† iteracion â”‚\n",
       "â”‚ l_accuracy â”† io         â”† _return    â”† nnual_Grow â”†   â”† ---     â”† ---    â”† e         â”† ---       â”‚\n",
       "â”‚ ---        â”† ---        â”† ---        â”† th_Rate    â”†   â”† f64     â”† i64    â”† ---       â”† i64       â”‚\n",
       "â”‚ f64        â”† f64        â”† f64        â”† ---        â”†   â”†         â”†        â”† i64       â”†           â”‚\n",
       "â”‚            â”†            â”†            â”† f64        â”†   â”†         â”†        â”†           â”†           â”‚\n",
       "â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 0.582418   â”† 0.077726   â”† 0.44833    â”† 0.64089    â”† â€¦ â”† 0.2     â”† 60     â”† 128       â”† 26        â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parametros_data_2025.sort(\"directional_accuracy\", descending=True).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc02e1e6",
   "metadata": {},
   "source": [
    "### Recuperando el Modelo y sus CaracterÃ­sticas del Modelo con mejor `directional_accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a00f7c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar el mejor modelo\n",
    "parametros_mejor_modelo = df_parametros_data_2025.sort(\"directional_accuracy\", descending=True).head(1).to_dicts()[0]\n",
    "\n",
    "modelo_comprobacion = modelos[parametros_mejor_modelo['iteracion']]\n",
    "instancia_dict = {\n",
    "    'input_dim': parametros_mejor_modelo['input_dim'],\n",
    "    'hidden_dims': parametros_mejor_modelo['hidden_dims'],\n",
    "    'dropout_prob': parametros_mejor_modelo['dropout'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761a3ac0",
   "metadata": {},
   "source": [
    "#### ComprobaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbbf4441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directional_accuracy --> 0.5824175824175825\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def comprobacion(model, scaler):\n",
    "\n",
    "    # Columnas Features\n",
    "    features_pytorch = [col for col in df_future.columns if col not in [\"date\", 'target_direction', 'price_tomorrow']]\n",
    "\n",
    "    # Modelo tal cual sera utilizado en Produccion\n",
    "    pytorch_model = model\n",
    "    pytorch_model.eval()\n",
    "\n",
    "    # device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Preparacion del X_input para PyTorch \n",
    "    \n",
    "    # X_input_pytorch = scaler.transform(df_future.sort('date').select(features_pytorch).to_numpy())\n",
    "    X_input_pytorch = X_test_future\n",
    "    X_input_pytorch_t = torch.tensor(X_input_pytorch, dtype=torch.float32)\n",
    "    # Inferencia \n",
    "    with torch.no_grad():\n",
    "        y_pred_pytorch_t = pytorch_model(X_input_pytorch_t.to(device))\n",
    "\n",
    "    preds = y_pred_pytorch_t.cpu().detach().numpy().flatten()  \n",
    "    y_test = copy.deepcopy(y_test_future)\n",
    "\n",
    "    aciertos_direccion = 0\n",
    "    for i in range(len(preds)):\n",
    "        if (preds[i] > 0.5 and y_test[i] == 1) or (preds[i] < 0.5 and y_test[i] == -1):\n",
    "            aciertos_direccion += 1\n",
    "  \n",
    "    directional_accuracy = aciertos_direccion / len(preds)\n",
    "    return directional_accuracy\n",
    "\n",
    "\n",
    "directional_data_2025 = comprobacion(modelo_comprobacion, scaler_global)\n",
    "print(f\"directional_accuracy --> {directional_data_2025}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9838a485",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### ðŸ’¾ Guardando los Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363723aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib \n",
    "\n",
    "torch.save(modelo_comprobacion.state_dict(), 'models/pytorch_model_data_2025_binario.pth')\n",
    "\n",
    "with open(\"models/config_pytorch_2025_binario.json\", \"w\") as f:\n",
    "    json.dump(instancia_dict, f, indent=4)\n",
    "\n",
    "scaler_filename = \"models/scaler_global_pytorch_binario.joblib\"\n",
    "joblib.dump(scaler_global, scaler_filename)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
