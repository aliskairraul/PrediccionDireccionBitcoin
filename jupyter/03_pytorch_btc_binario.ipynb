{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eec7df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Tus importaciones originales se mantienen\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5736e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"Fija la semilla para todas las librerías relevantes para la reproducibilidad.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "        \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb44dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df: pl.DataFrame, columnas: list) -> pl.DataFrame:        \n",
    "    \"\"\"... (código original sin cambios) ...\"\"\"\n",
    "    # Lags para features de mercado\n",
    "    for column in columnas:\n",
    "        if column in ['date', 'price']:\n",
    "            continue\n",
    "        df = df.with_columns(pl.col(column).shift(1).alias(f'{column}_lag_1'))\n",
    "\n",
    "    # **CALCULAR _change_1d\n",
    "    for column in columnas:\n",
    "        if column in ['date', 'total_volume']:\n",
    "            continue\n",
    "        df = df.with_columns(((pl.col(column) - pl.col(column).shift(1)) / pl.col(column).shift(1)).alias(f\"{column}_change_1d\")  )     \n",
    "\n",
    "    # Ratios intermercado\n",
    "    for column in columnas:\n",
    "        if column in ['date', 'price', 'total_volume']:\n",
    "            continue\n",
    "        df = df.with_columns((pl.col(\"price\") / pl.col(column)).alias(f\"btc_{column}_ratio\"))\n",
    "\n",
    "    df = df.with_columns([\n",
    "        pl.when(pl.col(\"price_change_1d\") > 0)\n",
    "          .then(1)\n",
    "          .when(pl.col(\"price_change_1d\") < 0)\n",
    "          .then(-1)\n",
    "          .otherwise(0)\n",
    "          .alias(\"price_direction_1d\"),\n",
    "    ])\n",
    "    # Features temporales básicas\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"date\").dt.year().alias(\"year\"),\n",
    "        pl.col(\"date\").dt.month().alias(\"month\"),\n",
    "        pl.col(\"date\").dt.day().alias(\"day\"),\n",
    "        pl.col(\"date\").dt.weekday().alias(\"weekday\"),\n",
    "    ])\n",
    "    # Lags de precio Variados\n",
    "    lags = [15, 30, 45, 60, 75, 90]\n",
    "    for lag in lags:\n",
    "        df = df.with_columns([\n",
    "            # Cambios en lags propuestos\n",
    "            pl.col(\"price\").shift(lag).alias(f\"btc_lag_{lag}\"),\n",
    "        ])\n",
    "    # Rolling statistics de precio\n",
    "    windows = [3, 7, 14, 21, 30]\n",
    "    for window in windows:\n",
    "        df = df.with_columns([\n",
    "            pl.col(\"price\").rolling_std(window).alias(f\"btc_std_{window}\"),\n",
    "        ])\n",
    "    windows = [30, 60, 90]\n",
    "    for window in windows:\n",
    "        df = df.with_columns([\n",
    "            # Medias de los Instrumentos en Ciertas Ventanas\n",
    "            pl.col(\"price\").rolling_mean(window).alias(f\"price_ma_{window}\"),\n",
    "            # Momentum adicional basado en precio (no confundir con price_change_1d)\n",
    "            ((pl.col(\"price\") / pl.col(f\"btc_lag_{window}\")) - 1).alias(f\"btc_momentum_{window}d\"),\n",
    "        ])\n",
    "    # **TARGET: precio de mañana (SIEMPRE AL FINAL)**\n",
    "    df = df.with_columns(pl.col(\"price\").shift(-1).alias(\"price_tomorrow\"))\n",
    "    df = df.with_columns(\n",
    "        pl.when((pl.col('price_tomorrow') - pl.col('price')) > 0)\n",
    "        .then(1)\n",
    "        .when((pl.col('price_tomorrow') - pl.col('price')) < 0)\n",
    "        .then(-1)\n",
    "        .otherwise(0)\n",
    "        .alias('target_direction')\n",
    "    )\n",
    "    # Eliminar NaNs (ÚLTIMO PASO)\n",
    "    max_offset = max(max(lags, default=0), max(windows, default=0), 1)\n",
    "    return (df.slice(max_offset, df.shape[0] - max_offset)).drop_nulls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98b35e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_split(df: pl.DataFrame, target: str, test_size: float = 0.4, fecha_corte: date = date(2024, 12, 31)):\n",
    "    # ... (Esta función también se mantiene casi igual, solo se añade el escalado de datos)\n",
    "    \"\"\"... (código original con la adición del scaler) ...\"\"\"\n",
    "    df_trainval = df.filter(pl.col(\"date\") <= fecha_corte)\n",
    "    df_future = df.filter(pl.col(\"date\") > fecha_corte)\n",
    "    df_trainval = df_trainval.sort(\"date\")\n",
    "    df_future = df_future.sort(\"date\")\n",
    "    split_idx = int(len(df_trainval) * (1 - test_size))\n",
    "    df_train = df_trainval.slice(0, split_idx)\n",
    "    df_test = df_trainval.slice(split_idx, len(df_trainval) - split_idx)\n",
    "    feature_cols = [col for col in df_train.columns if col not in [\"date\", target, 'price_tomorrow']]\n",
    "\n",
    "    # --- ¡NUEVO Y MUY IMPORTANTE PARA REDES NEURONALES! ---\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(df_train.select(feature_cols).to_numpy())\n",
    "    X_test = scaler.transform(df_test.select(feature_cols).to_numpy())\n",
    "    X_test_future = scaler.transform(df_future.select(feature_cols).to_numpy())\n",
    "    # --- FIN DE LA SECCIÓN NUEVA ---\n",
    "\n",
    "    y_train = df_train.select(target).to_numpy().flatten()\n",
    "    y_test = df_test.select(target).to_numpy().flatten()\n",
    "    y_test_future = df_future.select(target).to_numpy().flatten()\n",
    "    return df_trainval, df_future, X_train, y_train, X_test, y_test, df_test, df_train, X_test_future, y_test_future, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf64c1a",
   "metadata": {},
   "source": [
    "## Definición del Modelo Pytorch\n",
    "#### `MLP (Multi-Layer Perceptron)`\n",
    "- **Capa Lineal (nn.Linear):**\n",
    "    - Realiza una transformación lineal (y=Wx+b). Es la capa que \"aprende\" las relaciones entre las features.\n",
    "- **Activación (nn.ReLU):** \n",
    "    - Introduce la no-linealidad. Sin esto, apilar capas lineales sería matemáticamente igual a tener una sola capa lineal, y el modelo no podría aprender patrones complejos. ReLU simplemente convierte todos los valores negativos en cero.\n",
    "- **Regularización (nn.Dropout):** \n",
    "    - Como vimos, combate el sobreajuste.\n",
    "- **Capa de Salida:** \n",
    "    - Es una capa lineal con una sola neurona para producir un único valor de salida (nuestra predicción de target_direction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e20c6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitcoinDirectionNet(nn.Module):\n",
    "    \"\"\"Red neuronal optimizada para predecir dirección del precio de Bitcoin.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims=[256, 128, 64], dropout_prob=0.3):\n",
    "        super(BitcoinDirectionNet, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Construcción dinámica de capas\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),  # BatchNorm para mejor estabilidad\n",
    "                nn.LeakyReLU(0.1),  # LeakyReLU para evitar neuronas muertas\n",
    "                nn.Dropout(dropout_prob)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Capa final de clasificación\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        layers.append(nn.Sigmoid())  # Sigmoid para probabilidades [0,1]\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db05be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_modelo_pytorch(\n",
    "    modelo: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    df_test: pl.DataFrame,\n",
    "    device: torch.device,\n",
    "    transaction_cost_pct: float = 0.001,\n",
    "    threshold: float = 0.5  # Umbral para clasificación\n",
    ") -> dict:\n",
    "    \"\"\"Función de evaluación adaptada para PyTorch.\"\"\"\n",
    "    modelo.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in data_loader:\n",
    "            features = features.to(device)\n",
    "            outputs = modelo(features)\n",
    "            probs = outputs.cpu().numpy().flatten()\n",
    "            preds = (probs > threshold).astype(int) * 2 - 1  # Convertir a -1/1\n",
    "            all_preds.extend(preds)\n",
    "            all_probs.extend(probs)\n",
    "\n",
    "    preds = np.array(all_preds)\n",
    "\n",
    "    df_test = df_test.sort('date')\n",
    "    prices_hoy = df_test['price'].to_numpy()\n",
    "    prices_tomorrow = df_test['price_tomorrow'].to_numpy()\n",
    "    y_test = df_test['target_direction'].to_numpy()  # Obtenemos y_test del df\n",
    "\n",
    "    # El resto de la lógica de cálculo de métricas es idéntica\n",
    "    aciertos_direccion = 0\n",
    "    retornos = []\n",
    "    retornos_porcentuales = []\n",
    "    tasas_libre_riesgos = []\n",
    "    for i in range(len(preds)):\n",
    "        retorno = abs(prices_tomorrow[i] - prices_hoy[i])\n",
    "        tasa_libre = prices_hoy[i] * transaction_cost_pct\n",
    "        tasas_libre_riesgos.append(tasa_libre)\n",
    "        if (preds[i] > 0.5 and y_test[i] == 1) or (preds[i] < 0.5 and y_test[i] == -1):\n",
    "            aciertos_direccion += 1\n",
    "            retornos.append(retorno)\n",
    "            retornos_porcentuales.append((retorno - tasa_libre) / prices_hoy[i])\n",
    "        else:\n",
    "            retornos.append(-retorno)\n",
    "            retornos_porcentuales.append(-(retorno + tasa_libre) / prices_hoy[i])\n",
    "\n",
    "    exce_retorno = np.array(retornos) - np.array(tasas_libre_riesgos)\n",
    "    sharpe_ratio = np.mean(exce_retorno) / np.std(exce_retorno) if np.std(exce_retorno) != 0 else 0\n",
    "    directional_accuracy = aciertos_direccion / len(preds)\n",
    "    n_evaluated_days = len(preds)\n",
    "    retornos_porcentuales_np = np.array(retornos_porcentuales)\n",
    "    cumulative_return = np.prod(1 + retornos_porcentuales_np) - 1\n",
    "    cagr = (1 + cumulative_return)**(365 / n_evaluated_days) - 1\n",
    "\n",
    "    return {\n",
    "        \"directional_accuracy\": directional_accuracy,\n",
    "        \"sharpe_ratio\": sharpe_ratio,\n",
    "        \"cumulative_return\": cumulative_return,\n",
    "        \"Compound_Annual_Growth_Rate\": cagr,\n",
    "        \"n_evaluated_days\": n_evaluated_days,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cbea4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrena_evalua_pytorch(\n",
    "    hidden_dim: int,\n",
    "    learning_rate: float,\n",
    "    dropout: float,\n",
    "    epochs: int,\n",
    "    batch_size: int,\n",
    "    train_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    future_loader: DataLoader,\n",
    "    df_test: pl.DataFrame,\n",
    "    df_future: pl.DataFrame,\n",
    "    device: torch.device,\n",
    "    iteracion: int = 0,\n",
    "    return_model: bool=False\n",
    ") -> dict:\n",
    "\n",
    "    input_dim = next(iter(train_loader))[0].shape[1]\n",
    "\n",
    "    # Definir la arquitectura exacta\n",
    "    hidden_dims = [hidden_dim, hidden_dim//2, hidden_dim//4]\n",
    "\n",
    "    # 1. Instanciar modelo, función de pérdida y optimizador\n",
    "    # model = BitcoinDirectionNet(input_dim, [hidden_dim, hidden_dim//2, hidden_dim//4], dropout).to(device)\n",
    "    model = BitcoinDirectionNet(input_dim, hidden_dims, dropout).to(device)\n",
    "    \n",
    "    # Cambiar a pérdida de clasificación binaria\n",
    "    criterion = nn.BCELoss()  # Binary Cross Entropy para clasificación\n",
    "    \n",
    "    # Optimizador con weight decay (regularización L2)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "    # Scheduler para ajustar learning rate\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)\n",
    "\n",
    "    # --- INICIALIZACIÓN DE EARLY STOPPING ---\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    # -----------------------------------------\n",
    "    data_epoch_loss = []\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Bucle de entrenamiento\n",
    "        model.train()  # Poner el modelo en modo de entrenamiento\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "        running_train_loss = 0.0 \n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            # Convertir labels a formato binario [0,1]\n",
    "            # Asumiendo que labels son -1/1, convertimos a 0/1\n",
    "            binary_labels = (labels > 0).float().unsqueeze(1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, binary_labels)\n",
    "\n",
    "            # Backward pass y optimización\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping para estabilidad\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "        # ------------------------  ELIMINAR ESTE BLOQUE SI VA EL DE ABAJO    \n",
    "        avg_epoch_loss = epoch_loss / batch_count\n",
    "        scheduler.step(avg_epoch_loss)\n",
    "\n",
    "\n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        diccionario_epoch_loss = {}\n",
    "        diccionario_epoch_loss['epoch'] = epoch\n",
    "        diccionario_epoch_loss['train_loss'] = round(avg_train_loss, 4)\n",
    "        diccionario_epoch_loss['lr'] = optimizer.param_groups[0]['lr']\n",
    "        data_epoch_loss.append(diccionario_epoch_loss)\n",
    "        # -----------------------------------------------------------------\n",
    "\n",
    "    model.eval()\n",
    "    if return_model:\n",
    "        return model, optimizer\n",
    "    \n",
    "    # 3. Evaluación\n",
    "    metricas_test = evaluar_modelo_pytorch(model, test_loader, df_test, device)\n",
    "    metricas_future = evaluar_modelo_pytorch(model, future_loader, df_future, device)\n",
    "\n",
    "    params = {\n",
    "        'input_dim': input_dim,\n",
    "        'hidden_dims': hidden_dims,\n",
    "        'learning_rate': learning_rate,\n",
    "        'dropout': dropout, \n",
    "        'epochs': epochs, \n",
    "        'batch_size': batch_size,\n",
    "        'iteracion': iteracion\n",
    "    }\n",
    "\n",
    "    epochs_loss = {\n",
    "        'data_epochs_loss': data_epoch_loss\n",
    "    }\n",
    "\n",
    "    return metricas_test | params, metricas_future | params, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49437995",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## Empieza el Flujo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42aeecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "df = pl.read_parquet(\"db/db.parquet\")\n",
    "target = 'target_direction'\n",
    "# ['date', 'price', 'total_volume', 'market_cap', 'price_gold', 'stock_index_dowjones', 'stock_index_sp500', 'rate_US10Y', 'stock_index_ni225']\n",
    "columns = ['date', 'price', 'total_volume', 'stock_index_sp500', 'price_gold', 'rate_US10Y', 'stock_index_ni225']\n",
    "df = df.select(columns)\n",
    "df = df.pipe(add_features, columns)\n",
    "\n",
    "df_trainval, df_future, X_train, y_train, X_test, y_test, df_test, df_train, X_test_future, y_test_future, scaler_global = df.pipe(temporal_split, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb56ecc9",
   "metadata": {},
   "source": [
    "### Determinar Dispositivo  (GPU si está disponible)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a55399a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130cc884",
   "metadata": {},
   "source": [
    "#### `hidden_dims (Tamaño de las Capas Ocultas)`\n",
    "- **Teoría:**\n",
    "    - Es el \"poder cerebral\" de tu red. Define cuántas neuronas hay en cada capa oculta. Más neuronas significan que la red tiene más capacidad para aprender patrones complejos.\n",
    "- **Recomendaciones:**\n",
    "    - No hay una regla de oro, pero para datos tabulares, empezar con valores entre 32 y 256 es común.\n",
    "    - Una arquitectura popular es la de \"embudo\", donde cada capa sucesiva es más pequeña (ej., [256, 128, 64]).\n",
    "    - El tamaño de la capa de entrada (el número de features) puede ser una buena guía para el tamaño de la primera capa oculta.\n",
    "- **Riesgos:**\n",
    "    - Valores muy altos (ej., > 512): Alto riesgo de sobreajuste. La red tiene tanta capacidad que puede memorizar cada dato de entrenamiento, incluido el ruido.\n",
    "    - Valores muy bajos (ej., < 32): Riesgo de subajuste (underfitting). La red no tiene suficiente \"poder\" para capturar la complejidad de los datos.\n",
    "\n",
    "#### `learning_rates (Tasa de Aprendizaje)`\n",
    "- **Teoría:** \n",
    "    - Es el tamaño del paso que da el optimizador al ajustar los pesos del modelo. Es quizás el hiperparámetro más importante.\n",
    "    - Imagina que estás bajando una montaña a ciegas. La tasa de aprendizaje es el tamaño de tus pasos.\n",
    "- **Recomendaciones:**\n",
    "    - Es un parámetro muy sensible. Los valores más comunes se mueven en un rango logarítmico: 0.01, 0.005, 0.001, 0.0005, 0.0001.\n",
    "    - Empezar con 0.001 es casi siempre una apuesta segura.\n",
    "- **Riesgos:**\n",
    "    - Valor muy alto (ej., > 0.01): Divergencia. Los pasos son tan grandes que el modelo \"salta\" por encima de la solución óptima una y otra vez, y la pérdida (el error) puede explotar en lugar de disminuir.\n",
    "    - Valor muy bajo (ej., < 0.0001): Convergencia muy lenta. El entrenamiento tomará una eternidad y el modelo podría quedarse atascado en una mala solución (mínimo local) porque sus pasos son demasiado tímidos para salir de ahí.\n",
    "\n",
    "#### `dropouts (Tasa de Abandono)`\n",
    "- **Teoría:**\n",
    "    - Es una técnica de regularización fundamental para combatir el sobreajuste. En cada paso de entrenamiento, \"apaga\" aleatoriamente un porcentaje de neuronas. Esto fuerza a la red a aprender de forma más robusta, sin depender excesivamente de unas pocas neuronas.\n",
    "- **Recomendaciones:**\n",
    "    - Valores comunes están entre 0.1 y 0.5.\n",
    "    - Un valor de 0.5 es agresivo pero muy efectivo. Un valor de 0.2 o 0.3 es un punto de partida más conservador.\n",
    "- **Riesgos:**\n",
    "    - Valor muy alto (ej., > 0.6): Riesgo de subajuste. Estás apagando tantas neuronas que la red no tiene suficiente capacidad para aprender adecuadamente durante el entrenamiento.\n",
    "    - Valor muy bajo (ej., < 0.1): El efecto de regularización es mínimo, por lo que el riesgo de sobreajuste sigue siendo alto.\n",
    "\n",
    "#### `epochs_list (Número de Épocas)`\n",
    "- **Teoría:** \n",
    "    - Una época es una pasada completa del modelo por todo el conjunto de datos de entrenamiento. Es la cantidad de veces que el modelo \"estudia\" los datos.\n",
    "- **Recomendaciones:**\n",
    "    - Esto depende mucho del tamaño del dataset y la complejidad del problema. Puede variar desde 20 hasta 200+.\n",
    "- **Práctica recomendada:**\n",
    "    - En lugar de fijar un número de épocas, se usa una técnica llamada \"Early Stopping\" (Detención Temprana). Monitoreas el rendimiento en un set de validación y detienes el entrenamiento cuando el rendimiento en ese set deja de mejorar, evitando así el sobreajuste.\n",
    "- **Riesgos:**\n",
    "    - Demasiadas épocas: Causa principal de sobreajuste. El modelo aprende perfectamente los datos de entrenamiento pero pierde su capacidad de generalizar a datos nuevos.\n",
    "    - Muy pocas épocas: Subajuste. El modelo no ha tenido tiempo suficiente para aprender los patrones.\n",
    "\n",
    "#### `batch_size (Tamaño del Lote)`\n",
    "- **Teoría:**\n",
    "    - Es el número de muestras de datos que el modelo ve antes de actualizar sus pesos.\n",
    "- **Recomendaciones:**\n",
    "    - Se suelen usar potencias de 2 por optimizaciones de hardware: 32, 64, 128, 256.\n",
    "    - 64 o 128 son puntos de partida excelentes.\n",
    "    - Depende de la memoria de tu GPU (VRAM). Si te da un error de \"out of memory\", debes reducir el batch_size.\n",
    "- **Riesgos:**\n",
    "    - Valor muy grande: El entrenamiento es más rápido, pero puede generalizar peor, ya que los ajustes de los pesos son menos frecuentes y más \"promediados\".\n",
    "    - Valor muy pequeño: El entrenamiento es más lento y los ajustes son más \"ruidosos\", lo que a veces puede ayudar a la generalización (actúa como una forma de regularización), pero puede hacer que la convergencia sea inestable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c6657",
   "metadata": {},
   "source": [
    "### Creación de Tensores y DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0df2f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a tensores de PyTorch\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32)\n",
    "X_future_t = torch.tensor(X_test_future, dtype=torch.float32)\n",
    "y_future_t = torch.tensor(y_test_future, dtype=torch.float32)\n",
    "\n",
    "# Crear Datasets\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "future_dataset = TensorDataset(X_future_t, y_future_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbffaa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probando 72 combinaciones de hiperparámetros de PyTorch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ebc0c4d9804a3ead80dc15583481b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entrenando modelos PyTorch:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hiperparámetros de PyTorch\n",
    "hidden_dims = [64, 128]\n",
    "learning_rates = [0.001, 0.002, 0.003]\n",
    "dropouts = [0.2, 0.3]\n",
    "epochs_list = [40, 50, 60, 70, 80, 90]\n",
    "batch_size = 128  # Fijo para este ejemplo\n",
    "\n",
    "combinaciones = list(itertools.product(hidden_dims, learning_rates, dropouts, epochs_list))\n",
    "print(f\"Probando {len(combinaciones)} combinaciones de hiperparámetros de PyTorch...\")\n",
    "\n",
    "data_test = []\n",
    "data_2025 = []\n",
    "modelos = []\n",
    "iteracion = 0\n",
    "for hidden, lr, drop, epochs in tqdm(combinaciones, desc=\"Entrenando modelos PyTorch\"):\n",
    "    # Crear DataLoaders para la iteración actual\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    future_loader = DataLoader(future_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    datos_dict_test, datos_dict_real, modelo = entrena_evalua_pytorch(\n",
    "        hidden_dim=hidden, \n",
    "        learning_rate=lr, \n",
    "        dropout=drop, \n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size, \n",
    "        train_loader=train_loader, \n",
    "        test_loader=test_loader,\n",
    "        future_loader=future_loader, \n",
    "        df_test=df_test, \n",
    "        df_future=df_future, \n",
    "        device=device,\n",
    "        iteracion=iteracion\n",
    "    )\n",
    "    data_test.append(datos_dict_test)\n",
    "    data_2025.append(datos_dict_real)\n",
    "    modelos.append(modelo)\n",
    "    \n",
    "    iteracion += 1\n",
    "\n",
    "df_parametros_data_test = pl.DataFrame(data_test)\n",
    "df_parametros_data_2025 = pl.DataFrame(data_2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc9f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndirectional_accuracy    ########   y   0.564103   con [price, total_volume, stock_index_sp500, price_gold, rate_US10Y, stock_index_ni225, stock_index_dowjones]\\n\\ndirectional_accuracy    ########   y   0.582418   con [price, total_volume, stock_index_sp500, price_gold,  rate_US10Y, stock_index_ni225]\\n\\ndirectional_accuracy    ########   y   0.545788   con [price, total_volume, stock_index_sp500, stock_index_ni225]\\n\\ndirectional_accuracy    ########   y   0.534799   con [price, total_volume, stock_index_sp500, price_gold]\\n\\ndirectional_accuracy    ########   y   0.538462   con [price, total_volume, stock_index_sp500, rate_US10Y]\\n\\ndirectional_accuracy    ########   y   0.567766   con [price, total_volume, stock_index_sp500]\\n\\ndirectional_accuracy    ########   y   0.555147   con [price, total_volume]\\n\\ndirectional_accuracy    ########   y   0.540441   con [price]\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "directional_accuracy   0.564103   con [price, total_volume, stock_index_sp500, price_gold, rate_US10Y, stock_index_ni225, stock_index_dowjones]\n",
    "\n",
    "directional_accuracy   0.582418   con [price, total_volume, stock_index_sp500, price_gold,  rate_US10Y, stock_index_ni225]\n",
    "\n",
    "directional_accuracy   0.545788   con [price, total_volume, stock_index_sp500, stock_index_ni225]\n",
    "\n",
    "directional_accuracy   0.534799   con [price, total_volume, stock_index_sp500, price_gold]\n",
    "\n",
    "directional_accuracy   0.538462   con [price, total_volume, stock_index_sp500, rate_US10Y]\n",
    "\n",
    "directional_accuracy   0.567766   con [price, total_volume, stock_index_sp500]\n",
    "\n",
    "directional_accuracy   0.555147   con [price, total_volume]\n",
    "\n",
    "directional_accuracy   0.540441   con [price]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09b0dc3",
   "metadata": {},
   "source": [
    "#### Estos son los parámetros del Modelo con Mejor `directional_acrruracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f8bf592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>directional_accuracy</th><th>sharpe_ratio</th><th>cumulative_return</th><th>Compound_Annual_Growth_Rate</th><th>n_evaluated_days</th><th>input_dim</th><th>hidden_dims</th><th>learning_rate</th><th>dropout</th><th>epochs</th><th>batch_size</th><th>iteracion</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>list[i64]</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>0.582418</td><td>0.077726</td><td>0.44833</td><td>0.64089</td><td>273</td><td>42</td><td>[64, 32, 16]</td><td>0.003</td><td>0.2</td><td>60</td><td>128</td><td>26</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 12)\n",
       "┌────────────┬────────────┬────────────┬────────────┬───┬─────────┬────────┬───────────┬───────────┐\n",
       "│ directiona ┆ sharpe_rat ┆ cumulative ┆ Compound_A ┆ … ┆ dropout ┆ epochs ┆ batch_siz ┆ iteracion │\n",
       "│ l_accuracy ┆ io         ┆ _return    ┆ nnual_Grow ┆   ┆ ---     ┆ ---    ┆ e         ┆ ---       │\n",
       "│ ---        ┆ ---        ┆ ---        ┆ th_Rate    ┆   ┆ f64     ┆ i64    ┆ ---       ┆ i64       │\n",
       "│ f64        ┆ f64        ┆ f64        ┆ ---        ┆   ┆         ┆        ┆ i64       ┆           │\n",
       "│            ┆            ┆            ┆ f64        ┆   ┆         ┆        ┆           ┆           │\n",
       "╞════════════╪════════════╪════════════╪════════════╪═══╪═════════╪════════╪═══════════╪═══════════╡\n",
       "│ 0.582418   ┆ 0.077726   ┆ 0.44833    ┆ 0.64089    ┆ … ┆ 0.2     ┆ 60     ┆ 128       ┆ 26        │\n",
       "└────────────┴────────────┴────────────┴────────────┴───┴─────────┴────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parametros_data_2025.sort(\"directional_accuracy\", descending=True).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc02e1e6",
   "metadata": {},
   "source": [
    "### Recuperando el Modelo y sus Características del Modelo con mejor `directional_accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a00f7c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar el mejor modelo\n",
    "parametros_mejor_modelo = df_parametros_data_2025.sort(\"directional_accuracy\", descending=True).head(1).to_dicts()[0]\n",
    "\n",
    "modelo_comprobacion = modelos[parametros_mejor_modelo['iteracion']]\n",
    "instancia_dict = {\n",
    "    'input_dim': parametros_mejor_modelo['input_dim'],\n",
    "    'hidden_dims': parametros_mejor_modelo['hidden_dims'],\n",
    "    'dropout_prob': parametros_mejor_modelo['dropout'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761a3ac0",
   "metadata": {},
   "source": [
    "#### Comprobación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbbf4441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directional_accuracy --> 0.5824175824175825\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def comprobacion(model, scaler):\n",
    "\n",
    "    # Columnas Features\n",
    "    features_pytorch = [col for col in df_future.columns if col not in [\"date\", 'target_direction', 'price_tomorrow']]\n",
    "\n",
    "    # Modelo tal cual sera utilizado en Produccion\n",
    "    pytorch_model = model\n",
    "    pytorch_model.eval()\n",
    "\n",
    "    # device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Preparacion del X_input para PyTorch \n",
    "    \n",
    "    # X_input_pytorch = scaler.transform(df_future.sort('date').select(features_pytorch).to_numpy())\n",
    "    X_input_pytorch = X_test_future\n",
    "    X_input_pytorch_t = torch.tensor(X_input_pytorch, dtype=torch.float32)\n",
    "    # Inferencia \n",
    "    with torch.no_grad():\n",
    "        y_pred_pytorch_t = pytorch_model(X_input_pytorch_t.to(device))\n",
    "\n",
    "    preds = y_pred_pytorch_t.cpu().detach().numpy().flatten()  \n",
    "    y_test = copy.deepcopy(y_test_future)\n",
    "\n",
    "    aciertos_direccion = 0\n",
    "    for i in range(len(preds)):\n",
    "        if (preds[i] > 0.5 and y_test[i] == 1) or (preds[i] < 0.5 and y_test[i] == -1):\n",
    "            aciertos_direccion += 1\n",
    "  \n",
    "    directional_accuracy = aciertos_direccion / len(preds)\n",
    "    return directional_accuracy\n",
    "\n",
    "\n",
    "directional_data_2025 = comprobacion(modelo_comprobacion, scaler_global)\n",
    "print(f\"directional_accuracy --> {directional_data_2025}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9838a485",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### 💾 Guardando los Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363723aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib \n",
    "\n",
    "torch.save(modelo_comprobacion.state_dict(), 'models/pytorch_model_data_2025_binario.pth')\n",
    "\n",
    "with open(\"models/config_pytorch_2025_binario.json\", \"w\") as f:\n",
    "    json.dump(instancia_dict, f, indent=4)\n",
    "\n",
    "scaler_filename = \"models/scaler_global_pytorch_binario.joblib\"\n",
    "joblib.dump(scaler_global, scaler_filename)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
